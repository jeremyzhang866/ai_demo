import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
import matplotlib.patches as mpatches

# æ¨¡æ‹ŸMHATokenToKVPoolçš„ç¼“å†²åŒºåˆ›å»º
def create_kv_cache_example():
    # é…ç½®å‚æ•°
    size = 100  # ç®€åŒ–ä¸º100ä¸ªtokenä½ç½®ï¼Œä¾¿äºæ¼”ç¤º
    page_size = 16
    head_num = 4  # ç®€åŒ–ä¸º4ä¸ªæ³¨æ„åŠ›å¤´
    head_dim = 8  # ç®€åŒ–ä¸º8ç»´ï¼Œä¾¿äºæ˜¾ç¤º
    layer_num = 3  # ç®€åŒ–ä¸º3å±‚
    
    # åˆ›å»ºKå’ŒVç¼“å†²åŒº
    k_buffer = []
    v_buffer = []
    
    for layer_id in range(layer_num):
        k_tensor = torch.zeros((size + page_size, head_num, head_dim), 
                              dtype=torch.float32, device="cpu")  # ä½¿ç”¨float32ä¾¿äºæ˜¾ç¤º
        v_tensor = torch.zeros((size + page_size, head_num, head_dim), 
                              dtype=torch.float32, device="cpu")
        k_buffer.append(k_tensor)
        v_buffer.append(v_tensor)
    
    print(f"åˆ›å»ºçš„KVç¼“å­˜ç»“æ„:")
    print(f"- æ€»å±‚æ•°: {layer_num}")
    print(f"- æ¯å±‚Kç¼“å­˜å½¢çŠ¶: {k_buffer[0].shape}")
    print(f"- æ¯å±‚Vç¼“å­˜å½¢çŠ¶: {v_buffer[0].shape}")
    print(f"- æ€»tokenä½ç½®æ•°: {size + page_size}")
    print(f"- æ³¨æ„åŠ›å¤´æ•°: {head_num}")
    print(f"- æ¯å¤´ç»´åº¦: {head_dim}")
    
    return k_buffer, v_buffer

def visualize_tensor_structure():
    """å¯è§†åŒ–tensorç»“æ„"""
    print("\n" + "="*60)
    print("ğŸ” KVç¼“å­˜Tensorç»“æ„å¯è§†åŒ–")
    print("="*60)
    
    # åˆ›å»ºä¸€ä¸ªå°è§„æ¨¡ç¤ºä¾‹ä¾¿äºå¯è§†åŒ–
    layer_num = 3
    token_positions = 12  # å‡å°‘ä½ç½®æ•°ä¾¿äºæ˜¾ç¤º
    head_num = 4
    head_dim = 6  # å‡å°‘ç»´åº¦ä¾¿äºæ˜¾ç¤º
    
    k_buffers = []
    v_buffers = []
    
    for layer_id in range(layer_num):
        k_tensor = torch.zeros(token_positions, head_num, head_dim)
        v_tensor = torch.zeros(token_positions, head_num, head_dim)
        k_buffers.append(k_tensor)
        v_buffers.append(v_tensor)
    
    print(f"ğŸ“Š å¯è§†åŒ–é…ç½®:")
    print(f"   - å±‚æ•°: {layer_num}")
    print(f"   - Tokenä½ç½®: {token_positions}")
    print(f"   - æ³¨æ„åŠ›å¤´æ•°: {head_num}")
    print(f"   - æ¯å¤´ç»´åº¦: {head_dim}")
    print(f"   - æ¯å±‚Kç¼“å­˜å½¢çŠ¶: {k_buffers[0].shape}")
    print(f"   - æ¯å±‚Vç¼“å­˜å½¢çŠ¶: {v_buffers[0].shape}")
    
    return k_buffers, v_buffers, layer_num, token_positions, head_num, head_dim

def simulate_kv_operations():
    """æ¨¡æ‹ŸKVç¼“å­˜çš„SETå’ŒGETæ“ä½œï¼Œå¹¶å¡«å……å®é™…æ•°æ®"""
    print("\n" + "="*60)
    print("ğŸš€ æ¨¡æ‹ŸKVç¼“å­˜æ“ä½œ")
    print("="*60)
    
    k_buffers, v_buffers, layer_num, token_positions, head_num, head_dim = visualize_tensor_structure()
    
    # æ¨¡æ‹Ÿå¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®
    batch_size = 2
    seq_len = 3
    layer_id = 1  # æ“ä½œç¬¬1å±‚
    
    print(f"\nğŸ“ æ¨¡æ‹Ÿåœºæ™¯:")
    print(f"   - Batch size: {batch_size}")
    print(f"   - åºåˆ—é•¿åº¦: {seq_len}")
    print(f"   - ç›®æ ‡å±‚: {layer_id}")
    
    # 1. ç”Ÿæˆæ¨¡æ‹Ÿçš„Kå’ŒVå€¼
    num_tokens = batch_size * seq_len
    torch.manual_seed(42)  # å›ºå®šéšæœºç§å­ä¾¿äºå¤ç°
    cache_k = torch.randn(num_tokens, head_num, head_dim) * 0.5 + torch.arange(num_tokens).float().unsqueeze(-1).unsqueeze(-1) * 0.1
    cache_v = torch.randn(num_tokens, head_num, head_dim) * 0.3 + torch.arange(num_tokens).float().unsqueeze(-1).unsqueeze(-1) * 0.2
    
    # 2. åˆ†é…å­˜å‚¨ä½ç½®
    loc = torch.tensor([2, 3, 4, 5, 6, 7])  # è¿ç»­ä½ç½®2-7
    
    print(f"\nğŸ’¾ SETæ“ä½œ:")
    print(f"   - å­˜å‚¨ä½ç½®: {loc.tolist()}")
    print(f"   - Kå€¼å½¢çŠ¶: {cache_k.shape}")
    print(f"   - Vå€¼å½¢çŠ¶: {cache_v.shape}")
    
    # 3. æ‰§è¡Œå­˜å‚¨æ“ä½œ
    k_buffers[layer_id][loc] = cache_k
    v_buffers[layer_id][loc] = cache_v
    
    # 4. æ˜¾ç¤ºå­˜å‚¨åçš„æ•°æ®
    print(f"\nğŸ“‹ å­˜å‚¨åçš„æ•°æ®é¢„è§ˆ:")
    for i, pos in enumerate(loc[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªä½ç½®
        print(f"   ä½ç½®{pos}:")
        print(f"     K[å¤´0]: {k_buffers[layer_id][pos, 0, :].round(decimals=2).tolist()}")
        print(f"     V[å¤´0]: {v_buffers[layer_id][pos, 0, :].round(decimals=2).tolist()}")
    
    # 5. æ¨¡æ‹ŸGETæ“ä½œ
    print(f"\nğŸ” GETæ“ä½œ:")
    query_positions = torch.tensor([2, 3, 4])
    retrieved_k = k_buffers[layer_id][query_positions]
    retrieved_v = v_buffers[layer_id][query_positions]
    
    print(f"   - æŸ¥è¯¢ä½ç½®: {query_positions.tolist()}")
    print(f"   - æ£€ç´¢åˆ°çš„Kå½¢çŠ¶: {retrieved_k.shape}")
    print(f"   - æ£€ç´¢åˆ°çš„Vå½¢çŠ¶: {retrieved_v.shape}")
    
    return k_buffers, v_buffers, loc, layer_id

def create_tensor_heatmap():
    """åˆ›å»ºtensorçš„çƒ­åŠ›å›¾å¯è§†åŒ–"""
    print("\n" + "="*60)
    print("ğŸ¨ åˆ›å»ºTensorçƒ­åŠ›å›¾å¯è§†åŒ–")
    print("="*60)
    
    k_buffers, v_buffers, loc, layer_id = simulate_kv_operations()
    
    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('KVç¼“å­˜Tensorå¯è§†åŒ– - ç¬¬1å±‚', fontsize=16, fontweight='bold')
    
    # 1. Kç¼“å­˜çƒ­åŠ›å›¾ - æ‰€æœ‰å¤´çš„å¹³å‡å€¼
    k_data = k_buffers[layer_id][:10, :, :].mean(dim=1).numpy()  # åªæ˜¾ç¤ºå‰10ä¸ªä½ç½®
    im1 = axes[0, 0].imshow(k_data, cmap='viridis', aspect='auto')
    axes[0, 0].set_title('Kç¼“å­˜çƒ­åŠ›å›¾ (æ‰€æœ‰å¤´å¹³å‡)', fontweight='bold')
    axes[0, 0].set_xlabel('ç‰¹å¾ç»´åº¦')
    axes[0, 0].set_ylabel('Tokenä½ç½®')
    axes[0, 0].set_yticks(range(10))
    axes[0, 0].set_yticklabels([f'ä½ç½®{i}' for i in range(10)])
    plt.colorbar(im1, ax=axes[0, 0])
    
    # æ ‡è®°å·²ä½¿ç”¨çš„ä½ç½®
    for pos in loc:
        if pos < 10:
            axes[0, 0].add_patch(Rectangle((0, pos-0.5), k_data.shape[1], 1, 
                                         fill=False, edgecolor='red', linewidth=2))
    
    # 2. Vç¼“å­˜çƒ­åŠ›å›¾ - æ‰€æœ‰å¤´çš„å¹³å‡å€¼
    v_data = v_buffers[layer_id][:10, :, :].mean(dim=1).numpy()
    im2 = axes[0, 1].imshow(v_data, cmap='plasma', aspect='auto')
    axes[0, 1].set_title('Vç¼“å­˜çƒ­åŠ›å›¾ (æ‰€æœ‰å¤´å¹³å‡)', fontweight='bold')
    axes[0, 1].set_xlabel('ç‰¹å¾ç»´åº¦')
    axes[0, 1].set_ylabel('Tokenä½ç½®')
    axes[0, 1].set_yticks(range(10))
    axes[0, 1].set_yticklabels([f'ä½ç½®{i}' for i in range(10)])
    plt.colorbar(im2, ax=axes[0, 1])
    
    # æ ‡è®°å·²ä½¿ç”¨çš„ä½ç½®
    for pos in loc:
        if pos < 10:
            axes[0, 1].add_patch(Rectangle((0, pos-0.5), v_data.shape[1], 1, 
                                         fill=False, edgecolor='red', linewidth=2))
    
    # 3. å•ä¸ªå¤´çš„Kç¼“å­˜è¯¦ç»†è§†å›¾
    k_head0 = k_buffers[layer_id][:10, 0, :].numpy()
    im3 = axes[1, 0].imshow(k_head0, cmap='coolwarm', aspect='auto')
    axes[1, 0].set_title('Kç¼“å­˜è¯¦ç»†è§†å›¾ (å¤´0)', fontweight='bold')
    axes[1, 0].set_xlabel('ç‰¹å¾ç»´åº¦')
    axes[1, 0].set_ylabel('Tokenä½ç½®')
    axes[1, 0].set_yticks(range(10))
    axes[1, 0].set_yticklabels([f'ä½ç½®{i}' for i in range(10)])
    plt.colorbar(im3, ax=axes[1, 0])
    
    # 4. å•ä¸ªå¤´çš„Vç¼“å­˜è¯¦ç»†è§†å›¾
    v_head0 = v_buffers[layer_id][:10, 0, :].numpy()
    im4 = axes[1, 1].imshow(v_head0, cmap='coolwarm', aspect='auto')
    axes[1, 1].set_title('Vç¼“å­˜è¯¦ç»†è§†å›¾ (å¤´0)', fontweight='bold')
    axes[1, 1].set_xlabel('ç‰¹å¾ç»´åº¦')
    axes[1, 1].set_ylabel('Tokenä½ç½®')
    axes[1, 1].set_yticks(range(10))
    axes[1, 1].set_yticklabels([f'ä½ç½®{i}' for i in range(10)])
    plt.colorbar(im4, ax=axes[1, 1])
    
    # æ·»åŠ å›¾ä¾‹
    red_patch = mpatches.Patch(color='red', label='å·²ä½¿ç”¨ä½ç½®')
    fig.legend(handles=[red_patch], loc='upper right')
    
    plt.tight_layout()
    plt.savefig('/home/zjm.zhang/zjm_workspace/ai_demo/pytorch_test/kv_cache_heatmap.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: kv_cache_heatmap.png")

def create_3d_visualization():
    """åˆ›å»º3Då¯è§†åŒ–"""
    print("\n" + "="*60)
    print("ğŸŒŸ åˆ›å»º3D Tensorå¯è§†åŒ–")
    print("="*60)
    
    k_buffers, v_buffers, loc, layer_id = simulate_kv_operations()
    
    # åˆ›å»º3Då›¾å½¢
    fig = plt.figure(figsize=(15, 10))
    
    # Kç¼“å­˜3Då¯è§†åŒ–
    ax1 = fig.add_subplot(121, projection='3d')
    
    # åªæ˜¾ç¤ºå·²ä½¿ç”¨çš„ä½ç½®
    used_positions = loc[:6]  # å‰6ä¸ªä½ç½®
    k_data = k_buffers[layer_id][used_positions, :, :].numpy()
    
    # åˆ›å»ºç½‘æ ¼
    positions, heads, dims = np.meshgrid(
        range(len(used_positions)), 
        range(k_data.shape[1]), 
        range(k_data.shape[2])
    )
    
    # ç»˜åˆ¶3Dæ•£ç‚¹å›¾
    colors = k_data.flatten()
    scatter = ax1.scatter(positions.flatten(), heads.flatten(), dims.flatten(), 
                         c=colors, cmap='viridis', alpha=0.6, s=20)
    
    ax1.set_xlabel('Tokenä½ç½®')
    ax1.set_ylabel('æ³¨æ„åŠ›å¤´')
    ax1.set_zlabel('ç‰¹å¾ç»´åº¦')
    ax1.set_title('Kç¼“å­˜3Då¯è§†åŒ–', fontweight='bold')
    
    # Vç¼“å­˜3Då¯è§†åŒ–
    ax2 = fig.add_subplot(122, projection='3d')
    
    v_data = v_buffers[layer_id][used_positions, :, :].numpy()
    colors_v = v_data.flatten()
    scatter_v = ax2.scatter(positions.flatten(), heads.flatten(), dims.flatten(), 
                           c=colors_v, cmap='plasma', alpha=0.6, s=20)
    
    ax2.set_xlabel('Tokenä½ç½®')
    ax2.set_ylabel('æ³¨æ„åŠ›å¤´')
    ax2.set_zlabel('ç‰¹å¾ç»´åº¦')
    ax2.set_title('Vç¼“å­˜3Då¯è§†åŒ–', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('/home/zjm.zhang/zjm_workspace/ai_demo/pytorch_test/kv_cache_3d.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… 3Då¯è§†åŒ–å·²ä¿å­˜åˆ°: kv_cache_3d.png")

def print_detailed_tensor_info():
    """æ‰“å°è¯¦ç»†çš„tensorä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ“Š è¯¦ç»†Tensorä¿¡æ¯")
    print("="*60)
    
    k_buffers, v_buffers, loc, layer_id = simulate_kv_operations()
    
    print(f"\nğŸ” ç¬¬{layer_id}å±‚è¯¦ç»†ä¿¡æ¯:")
    print(f"   Kç¼“å­˜tensor:")
    print(f"     - å½¢çŠ¶: {k_buffers[layer_id].shape}")
    print(f"     - æ•°æ®ç±»å‹: {k_buffers[layer_id].dtype}")
    print(f"     - è®¾å¤‡: {k_buffers[layer_id].device}")
    print(f"     - å†…å­˜å ç”¨: {k_buffers[layer_id].numel() * k_buffers[layer_id].element_size()} bytes")
    print(f"     - éé›¶å…ƒç´ æ•°: {torch.count_nonzero(k_buffers[layer_id])}")
    
    print(f"\n   Vç¼“å­˜tensor:")
    print(f"     - å½¢çŠ¶: {v_buffers[layer_id].shape}")
    print(f"     - æ•°æ®ç±»å‹: {v_buffers[layer_id].dtype}")
    print(f"     - è®¾å¤‡: {v_buffers[layer_id].device}")
    print(f"     - å†…å­˜å ç”¨: {v_buffers[layer_id].numel() * v_buffers[layer_id].element_size()} bytes")
    print(f"     - éé›¶å…ƒç´ æ•°: {torch.count_nonzero(v_buffers[layer_id])}")
    
    # æ˜¾ç¤ºå…·ä½“æ•°å€¼
    print(f"\nğŸ“‹ å·²ä½¿ç”¨ä½ç½®çš„å…·ä½“æ•°å€¼:")
    for i, pos in enumerate(loc[:3]):
        print(f"\n   ä½ç½®{pos} (Token {i}):")
        print(f"     æ‰€æœ‰å¤´çš„Kå€¼:")
        for head in range(k_buffers[layer_id].shape[1]):
            k_vals = k_buffers[layer_id][pos, head, :].round(decimals=2)
            print(f"       å¤´{head}: {k_vals.tolist()}")
        
        print(f"     æ‰€æœ‰å¤´çš„Vå€¼:")
        for head in range(v_buffers[layer_id].shape[1]):
            v_vals = v_buffers[layer_id][pos, head, :].round(decimals=2)
            print(f"       å¤´{head}: {v_vals.tolist()}")

def demonstrate_attention_computation():
    """æ¼”ç¤ºattentionè®¡ç®—è¿‡ç¨‹"""
    print("\n" + "="*60)
    print("ğŸ§  æ¼”ç¤ºAttentionè®¡ç®—è¿‡ç¨‹")
    print("="*60)
    
    k_buffers, v_buffers, loc, layer_id = simulate_kv_operations()
    
    # æ¨¡æ‹Ÿä¸€ä¸ªquery
    torch.manual_seed(123)
    query = torch.randn(1, 4, 6) * 0.5  # [1, head_num, head_dim]
    
    print(f"ğŸ” Query tensor:")
    print(f"   - å½¢çŠ¶: {query.shape}")
    print(f"   - å€¼: {query.round(decimals=2)}")
    
    # è·å–å·²å­˜å‚¨çš„Kå’ŒV
    used_positions = loc[:4]  # ä½¿ç”¨å‰4ä¸ªä½ç½®
    keys = k_buffers[layer_id][used_positions]    # [4, 4, 6]
    values = v_buffers[layer_id][used_positions]  # [4, 4, 6]
    
    print(f"\nğŸ“š ä»ç¼“å­˜è·å–çš„Kå’ŒV:")
    print(f"   - Kå½¢çŠ¶: {keys.shape}")
    print(f"   - Vå½¢çŠ¶: {values.shape}")
    
    # è®¡ç®—attention scores
    # query: [1, 4, 6], keys: [4, 4, 6] -> scores: [1, 4, 4]
    scores = torch.matmul(query, keys.transpose(-2, -1))  # [1, 4, 6] @ [4, 6, 4] -> [1, 4, 4]
    
    print(f"\nğŸ¯ Attention Scores:")
    print(f"   - å½¢çŠ¶: {scores.shape}")
    print(f"   - å€¼ (å¤´0): {scores[0, 0, :].round(decimals=2).tolist()}")
    print(f"   - å€¼ (å¤´1): {scores[0, 1, :].round(decimals=2).tolist()}")
    
    # åº”ç”¨softmax
    attention_weights = torch.softmax(scores, dim=-1)
    
    print(f"\nâš–ï¸ Attention Weights (softmaxå):")
    print(f"   - å½¢çŠ¶: {attention_weights.shape}")
    print(f"   - æƒé‡ (å¤´0): {attention_weights[0, 0, :].round(decimals=3).tolist()}")
    print(f"   - æƒé‡ (å¤´1): {attention_weights[0, 1, :].round(decimals=3).tolist()}")
    
    # è®¡ç®—æœ€ç»ˆè¾“å‡º
    # attention_weights: [1, 4, 4], values: [4, 4, 6] -> output: [1, 4, 6]
    output = torch.matmul(attention_weights, values)
    
    print(f"\nğŸŠ æœ€ç»ˆè¾“å‡º:")
    print(f"   - å½¢çŠ¶: {output.shape}")
    print(f"   - å€¼ (å¤´0): {output[0, 0, :].round(decimals=2).tolist()}")
    print(f"   - å€¼ (å¤´1): {output[0, 1, :].round(decimals=2).tolist()}")

# åˆ›å»ºç¼“å­˜
k_buffer, v_buffer = create_kv_cache_example()

# è¿è¡Œæ‰€æœ‰å¯è§†åŒ–å‡½æ•°
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹KVç¼“å­˜å®Œæ•´å¯è§†åŒ–æ¼”ç¤º...")
    
    # 1. åŸºç¡€ç»“æ„å¯è§†åŒ–
    visualize_tensor_structure()
    
    # 2. æ“ä½œæ¼”ç¤º
    simulate_kv_operations()
    
    # 3. åˆ›å»ºçƒ­åŠ›å›¾
    create_tensor_heatmap()
    
    # 4. åˆ›å»º3Då¯è§†åŒ–
    create_3d_visualization()
    
    # 5. è¯¦ç»†ä¿¡æ¯
    print_detailed_tensor_info()
    
    # 6. Attentionè®¡ç®—æ¼”ç¤º
    demonstrate_attention_computation()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰å¯è§†åŒ–å®Œæˆï¼")
    print("ğŸ“ å›¾ç‰‡å·²ä¿å­˜åˆ°å½“å‰ç›®å½•:")
    print("   - kv_cache_heatmap.png (çƒ­åŠ›å›¾)")
    print("   - kv_cache_3d.png (3Då¯è§†åŒ–)")
    print("="*60)